%---Matrix-Vector System---------------------------------------------------------
\section{Matrix-Vector System}
\label{sec:matrix_vect_syst}
We will now transform the variational form (\ref{eq:discrete_var_form_elliptic}) into Matrix-Vector system form. 
To do so let
$r \in \mathbb{N}$ denote the polynomial degree and consequently the element degree of freedom.
Note that in this thesis we will only consider global polynomial degrees, meaning one set polynomial degree for all elements.
Next let $\{\Phi_0,\ldots,\Phi_M\}$ be a basis of $V_h$, where $M = \dim(V_h)$.
We can represent the sought Galerkin approximation as $u_h = \sum_{j=0}^{M} \alpha_j \Phi_j\in V_h$ for coefficients
$\alpha_j \in \mathbb{R}$. Then (\ref{eq:discrete_var_form_elliptic}) is equivalent to:
\begin{equation*}
	\sum_{j=0}^{M} \alpha_j b_h(\Phi_j, \Phi_i) = \ell_h(\Phi_i) \qquad \forall i=0,\ldots,M,
\end{equation*}
which corresponds to the system:
\begin{equation}
	\label{eq:fully_discrete_dg_system_elliptic}
	\textbf{Bu} = \textbf{l}
\end{equation}
for $ \textbf{B} \in \mathbb{R}^{M\times M}, [\textbf{B}]_{i,j} = b_h(\Phi_j, \Phi_i),
	\textbf{u} \in \mathbb{R}^M, [\textbf{u}]_j = \alpha_j,
	\textbf{l}\in\mathbb{R}^M, [\textbf{l}]_j = \ell_h(\Phi_j)$.

%---Basis-of-FE-Space------------------------------------------------------------
\section{Basis of Finite Element Space}
\label{sec:ell_basis}
There are many ways of choosing basis functions for finite element spaces. In this thesis we soley focus on elementwise nodal Lagrangian basis functions, although there are alternatives
which might be just as valid depending on the goal one pursues, like for example a modal Legendre basis.
Having decided to work with a Lagrange nodal basis means we need to decide on the placing of the nodes. In principle for a desired polynomial degree $r$ we 
could decide any $r+1$ points per elements. Since we restrict ourselves to a single global polynomial degree (i.e.\ no p-adaptivity) 
it makes sense to choose the same type of placement for all elements since this will allow us to calculate the actual function values on a single reference element (see below). \\
Here in contrast to continuous FEM, DG provides additional liberty, since there are no shared nodes over element faces we actually could decide to place the basis nodes purely inside of the element.
\\ \\
In this thesis we will not try to justify the choice of basis function too much and use Gauss-Lobatto quadrature nodes as basis nodes.
This is a commonly used nodal basis in continuous FEM and we will also use it for our DG method. In Appendix A.2.3 of \cite{diPietro2012} the \textit{Fekete points}, which 
in 1d coincide with the Gauss-Lobatto points, are recommended 
with the argument that this nodal basis yields a mass matrix with an optimal condition number. 
For more detailed information on choosing basis functions and alternative approaches see for example
Appendix A.2 in \cite{diPietro2012}. \\ \\
Let $n\in \{1,\ldots,N\}$ and $I_n \in \mathcal{T}_h$ be an arbitrary element.
We denote $\hat{I} = (-1,1)$ the \textit{reference element} and $\displaystyle F_n : \hat{I} \to I_n, \, \xi \mapsto \frac{x_n + x_{n+1}}{2} + \frac{h_n}{2} \xi $
the \textit{element map}. This now allows us to define a basis on the reference element and
extend it to all elements using the element map. \\
For a fixed polynomial degree $r \geq 2$ let $\xi_0,\ldots,\xi_{r} \in [-1,1]$ be the
Gauss-Lobatto nodes. \\
\begin{equation*}
	\begin{tabular}{c|c}
		\centering
		$r = 2$ & $\{-1, 1\}$                                         \\
		$r = 3$ & $\{-1,0, 1\}$                                       \\
		$r = 4$ & $\{-1,-\frac{1}{\sqrt{5}}, \frac{1}{\sqrt{5}}, 1\}$ \\
		\vdots  & \vdots                                              \\
	\end{tabular}
\end{equation*}
The inner nodes are given by the roots of $L_{r-1}^{\prime}$, the derivative of the $r-1$-th Legendre polynomial.
We define the basis on the reference element as the Lagrangian nodal basis

\begin{equation}
	\label{def:lagrange_ref_basis}
	\widehat{\phi}_i(\xi) := \prod_{\substack{j = 0 \\ j\neq i}}^{r}\frac{\xi - \xi_j}{\xi_i - \xi_j},
	\qquad \text{for } i=0,\ldots,r
\end{equation}
and define the basis functions on the element $I_n$ as
\begin{equation}
	\phi^n_i : I_n \to \mathbb{R}, \quad \phi^n_i(x) := \widehat{\phi}_i(F_n^{-1}(x))
	\nonumber
\end{equation}
as a last step we extend the basis functions to the whole domain $\Omega$ by zero
\begin{equation}
	\Phi_i^n: \Omega \to \mathbb{R}, \quad \Phi_i^n(x) :=
	\begin{cases}
		\phi_i^n(x), & \text{for } x\in I_n \\
		0,           & \text{else}
	\end{cases}
	\label{def:lagrange_basis_of_Vh}
\end{equation}
for $n=0,\ldots,N$ and $i=0,\ldots,r$. Clearly we have $\text{span}( \widehat{\phi}_0, \ldots, \widehat{\phi}_r) = \mathcal{P}^r(\hat{I})$
and by extension $\text{span}(\Phi_0^0,\ldots,\Phi_r^N) = V_h^r(\mathcal{T}_h)$. It is essential
that our basis has only local support, meaning the basis functions are zero on most of the domain. This is the key property which
allows the final matrices to be sparse. Choosing basis functions with global support, would lead to an unfeasible computational cost for
small mesh sizes. \\
By having chosen a Lagrangian nodal basis the mesh nodes exactly coincide with the Gauss-Lobatto nodes on each
element.
To simplify the notation we introduce a \textit{local-to-global} index map
\begin{equation}
	\label{def:local_to_global_map}
	T: \{0,\ldots,N\} \times \{0,\ldots,r\} \to \{1,\ldots,M\}
\end{equation}
where $M = (r+1)(N+1) = \dim(V_h)$. $T$ takes an element index $n$ and a local basis function index $i$ as inputs
and returns the globally assigned node index $T(n,i)$. $T$ corresponds to the
\textit{connectivity matrix}. In the simplest case we have the global index ordered from left to right and get
$T(n,i) = nr + i$.

\section{Quadrature Rule}
\label{sec:quadrature_rule}
To discuss the assembly of the components of the system (\ref{eq:fully_discrete_dg_system_elliptic}) in detail we will require a tool to approximate the integrals appearing in it, i.e.\ 
we need to choose a quadrature rule. The quadrature rule should primarily be as accurate as possible for the least amount of nodes necessary. So immediately one thinks of the Gauss-Legendre
quadrature rule, which for $r+1$ quadrature nodes, is exact for polynomials of degree $2r+1$. In fact Gauss-Legendre is often the preferred choice due to it's high accuracy. A downside
of Gauss-Legendre is that it does not include element boundary nodes. We do require the values of our basis functions at the boundary nodes for the consistency, symmetry and penalty terms in 
(\ref{eq:discrete_var_form_elliptic}), as well as the values of $c$ at the element faces. All these values can be preloaded at the desired quadrature nodes, which makes it simpler to use
a quadrature rule with nodes at the element boundary. This is why we choose the Gauss-Lobatto quadrature rule despite it's lower accuracy of being exact for for polynomials of degree $2r-1$ 
with $r+1$ quadrature nodes. \\
We could still use Gauss-Legendre and interpolate the element boundary values. Suppose we chose $r+1$ Gauss-Legendre quadrature nodes per element, then we require the element boundary values as well,
so after all we need to preload the values of the basis functions and the coefficient $c$ at $r+3$ nodes per element (the quadrature nodes plus the boundary nodes). 
If instead we choose $r+2$ Gauss-Lobatto quadrature nodes we achieve the same accuracy but use a node less.  \\
An additional benefit of using Gauss-Lobatto quadrature nodes is that now the mesh nodes as well as the quadrature nodes are of the same type. This simplifies the program architecture and allows 
for the possibility of using \textit{mass-lumping}, meaning we can let the basis nodes and the quadrature nodes coincide when assembling the mass matrix 
\begin{equation*}
	[\mathbf{M}]_{i,j} = (\Phi_j, \Phi_i)_{L^2(\Omega)}.
\end{equation*} 
When we use $r+1$ Gauss-Lobatto nodes to approximate the integral $\int_{\Omega} \Phi_j \Phi_i \text{ d}x$ we introduce an error, since $ \Phi_j \Phi_i \in \mathcal{P}^{2r}$, but the resulting 
mass-lumped mass matrix is diagonal, which is a crucial benefit in explicit time-marching schemes with continuous FEM spacial discretization. We compare the error created by using the inexact 
$r+1$ node Gauss-Lobatto quadrature rule for $\mathcal{P}^r$-elements with a higher order Gauss-Lobatto quadrature rule experimentally in \ref{sec:elliptic_numerical_experiments}, but we can already 
say that due to the decoupled element structure DG provides, the mass matrix is already block-diagonal and mass lumping provides no real benefit. \\ \\
All of this being said the choice of the Gauss-Lobatto quadrature rule is in our case mainly preferential and not the focus of this thesis.  


%---Matrix Assembly-------------------------------------------------------------------
\section{Stiffness Matrix Assembly}
\label{sec:stiff_assembly}
With the basis functions in (\ref{def:lagrange_basis_of_Vh}) defined we can now in detail investigate how
to assemble the matrix $\textbf{B}$ in (\ref{eq:fully_discrete_dg_system_elliptic}). To do so we firstly separate
the bilinear form $b_h$ into different components

\begin{align*}
	a_h(u,v)                & := \sum_{n=0}^N \int_{I_n} cu'v'\, \text{d}x                              \\
	b_h^{\text{cons}}(u,v)  & := \sum_{n=0}^{N+1} \avg{c(x_n)u'(x_n)}\jump{v(x_n)} + \avg{c(x_n)v'(x_n)}\jump{u(x_n)} \\
	b_h^{\text{penal}}(u,v) & := \sum_{n=0}^{N+1} \texttt{a}_n \jump{u(x_n)} \jump{v(x_n)}                          \\
\end{align*}

Let $\displaystyle u_h = \sum_{m=0}^{N} \sum_{j=0}^{r} \alpha_j^m \Phi_j^m \in V_h$ denote
the Galerkin approximation, then as discussed in section \ref{sec:ell_basis} the discrete variational formulation (\ref{eq:discrete_var_form_elliptic}) is equivalent
to
\begin{equation}
	\sum_{m=0}^{N} \sum_{j=0}^{r} \alpha_j^m \Big(
	a_h(\Phi_j^m,\Phi_i^n) - b_h^{\text{cons}}(\Phi_j^m,\Phi_i^n) + b_h^{\text{penal}}(\Phi_j^m,\Phi_i^n)
	\Big)
	= \ell_h(\Phi_i^n), \qquad \forall n=0,\ldots,N, i=0,\ldots,r
\end{equation}
which corresponds to the matrix vector system (\ref{eq:fully_discrete_dg_system_elliptic})
where we can write
\begin{equation*}
	\textbf{B} = \textbf{A} - \textbf{B}_{\text{cons}} + \textbf{B}_{\text{penal}}
\end{equation*}
we will assemble the three (symmetric) matrices separately.
\begin{align*}
	 & [\textbf{B}]_{T(n,i),T(m,j)} = b_h(\Phi_j^m, \Phi_i^n),
	 & [\textbf{A}]_{T(n,i),T(m,j)} = a_h(\Phi_j^m, \Phi_i^n)                                \\
	 & [\textbf{B}_{\text{cons}}]_{T(n,i),T(m,j)} = b_h^{\text{cons}} (\Phi_j^m, \Phi_i^n)
	 & [\textbf{B}_{\text{penal}}]_{T(n,i),T(m,j)} = b_h^{\text{penal}} (\Phi_j^m, \Phi_i^n)
\end{align*}
where $T$ is given by (\ref{def:local_to_global_map})

\subsection{Assembly of A}
$\textbf{A}$ is assembled similarly to the standard stiffness matrix in continuous finite element. The main difference is that there is no overlap in the elementwise contributions.
Each set of local (element) basis functions only contributes to the integrals over said element. 
We can rewrite $\textbf{A} = \sum_{s=0}^{N} \textbf{A}^{(s)}$, where
$[\textbf{A}^{(s)}]_{T(n,i),T(m,j)} = \int_{I_s} c \,(\Phi_j^m)' (\Phi_i^n)' \, \text{d}x$.
Now since we have $\text{supp}(\Phi_i^n)\subset I_n$ the only non-zero entries of $\textbf{A}^{(s)} $ are the ones where both $n=m=s$. Pulling back the integral
to the reference element using the chain rule and the substitution $F_s^{-1}(x) = \xi$ we find
\begin{equation*}
	\int_{I_s} c(x) \,(\Phi_j^s)'(x) (\Phi_i^s)'(x) \, \text{d}x =
	\frac{2}{h_s} \int_{\hat{I}} c(F_s(\xi)) \, \widehat{\phi}_j^{\, \prime}(\xi) \widehat{\phi}_i^{\, \prime}(\xi) \, \text{d}\xi
\end{equation*}
This integral now only depends on the reference shape functions, the element length $h_s$
and the values of the coefficient $c$. Using a higher order Gauss-Lobatto quadrature rule we can approximate the integral
only requiring the values of $c$ at the quadrature nodes, whilst having the values of $\phi, \phi'$ preloaded.
The total assembly of $\textbf{A}$ can therefore be achieved by calculating a local contribution matrix
$\widehat{\textbf{A}}^{(s)} \in \mathbb{R}^{(r+1)\times (r+1)}$ for each element $I_s$ and adding it into $\textbf{A}$.
\begin{example}
	For $c\equiv 1$ with $\mathcal{P}^1$-elements $(r=1)$ we have
	\begin{equation*}
		\widehat{\textbf{A}}^{(s)} = \frac{1}{h_s}
		\begin{bmatrix}
			1  & -1 \\
			-1 & 1
		\end{bmatrix}
	\end{equation*}
\end{example}

\subsection{Assembly of B consistency part}
\label{subsec:assembly_cons}
As before we rewrite $\textbf{B}_{\text{cons}} = \sum_{s=0}^{N+1} \textbf{B}_{\text{cons}}^{(s)} $, where

\begin{equation*}
	[\textbf{B}_{\text{cons}}^{(s)}]_{T(n,i),T(m,j)} = \avg{c(x_s) \Phi_j^{m \, \prime} (x_s)}\jump{\Phi_i^n(x_s)} + 
	\avg{c(x_s)\Phi_i^{n\, \prime} (x_s)}\jump{\Phi_j^m(x_s)}
\end{equation*}

\subsubsection{Interior Faces}
First let $s \in \{1,\ldots,N\}$ denote an interior face, we observe again, that the entries of
$\textbf{B}_{\text{cons}}^{(s)}$ can only be non-zero for an index tuple $(T(n,i),T(m,j))$ if
$n,m \in \{s, s-1\}$ by the local support of the basis functions.
Therefore assembling $ \textbf{B}_{\text{cons}}$ again comes down to calculating a local contribution matrix

\begin{equation*}
	\widehat{\textbf{B}}_{\text{cons}}^{(s)} =
	\begin{bmatrix}
		\textbf{C}_{\text{cons}}^{(s-1,s-1)} & \textbf{C}_{\text{cons}}^{(s-1,s)} \\
		\textbf{C}_{\text{cons}}^{(s,s-1)}   & \textbf{C}_{\text{cons}}^{(s,s)}
	\end{bmatrix}
	\in \mathbb{R}^{2(r+1) \times 2(r+1)}
\end{equation*}
for each interior face $x_s$ consisting of four blocks we will now lay out in more detail.
We discuss the boundary case separately.
Using again the local support of the basis functions we find
\begin{align*}
	 & [\textbf{C}_{\text{cons}}^{(s-1,s-1)}]_{i,j} = \frac{c(x_s^-)}{2} \Phi_j^{s-1\, \prime}(x_s^-) \Phi_i^{s-1}(x_s^-) + \frac{c(x_s^-)}{2} \Phi_i^{s-1\, \prime}(x_s^-) \Phi_j^{s-1}(x_s^-) \\
	 & [\textbf{C}_{\text{cons}}^{(s-1,s)}]_{i,j} = \frac{c(x_s^+)}{2} \Phi_j^{s\, \prime}(x_s^+) \Phi_i^{s-1}(x_s^-) - \frac{c(x_s^-)}{2} \Phi_i^{s-1\, \prime}(x_s^-) \Phi_j^{s}(x_s^+)       \\
	 & [\textbf{C}_{\text{cons}}^{(s,s)}]_{i,j} = -\frac{c(x_s^+)}{2} \Phi_j^{s\, \prime}(x_s^+) \Phi_i^{s}(x_s^+) - \frac{c(x_s^+)}{2} \Phi_i^{s\, \prime}(x_s^+) \Phi_j^{s}(x_s^+)
\end{align*}
where we have used the definitions of jump and average in (\ref{def:jump_average}).
Note that $ \textbf{C}_{\text{cons}}^{(s-1,s)} = (\textbf{C}_{\text{cons}}^{(s,s-1)})^T$ by the symmetry of the bilinear form $b_h^{\text{cons}}$.
Next we represent the values of the basis functions $ \Phi $ at the element boundary by the values of the reference
shape functions $\widehat{\phi}$
\begin{align}
	 & \Phi_i^s(x_s^+) = \widehat{\phi}_i(-1) ,                                                   & \Phi^{s-1}_i(x_s^-) = \widehat{\phi}_i(1) \label{eq:shape_fun_and_basis_fun_values_equality} \\
	 & \Phi_i^{s\, \prime }(x_s^+) = \frac{2}{h_{s}} \widehat{\phi}_i^{\,\prime}(-1),
	 & \Phi_i^{s-1\, \prime }(x_s^-) = \frac{2}{h_{s-1}} \widehat{\phi}_i^{\,\prime}(1) \nonumber
\end{align}
which finally yields
\begin{align*}
	 & [\textbf{C}_{\text{cons}}^{(s-1,s-1)}]_{i,j} = \frac{c(x_s^-)}{h_{s-1}} \widehat{\phi}_j^{\,\prime}(1) \widehat{\phi}_i(1) + \frac{c(x_s^-)}{h_{s-1}} \widehat{\phi}_i^{\,\prime} (1) \widehat{\phi}_j(1)  \\
	 & [\textbf{C}_{\text{cons}}^{(s-1,s)}]_{i,j} = \frac{c(x_s^+)}{h_{s}} \widehat{\phi}_j^{\,\prime} (-1) \widehat{\phi}_i (1) - \frac{c(x_s^-)}{h_{s-1}} \widehat{\phi}_i^{\,\prime} (1) \widehat{\phi}_j (-1) \\
	 & [\textbf{C}_{\text{cons}}^{(s,s)}]_{i,j} = -\frac{c(x_s^+)}{h_s} \widehat{\phi}_j^{\,\prime} (-1) \widehat{\phi}_i (-1) - \frac{c(x_s^+)}{h_s} \widehat{\phi}_i^{\,\prime} (-1) \widehat{\phi}_j (-1)
\end{align*}
\begin{example}
	Consider $c\equiv 1$ for $\mathcal{P}^1$-elements $(r=1)$
	with an equidistant mesh with meshsize $h$ we have
	\begin{equation*}
		\widehat{\textbf{B}}_{\text{cons}}^{(s)} = \frac{1}{h}
		\begin{bmatrix}
			0    & -1/2 & 1/2  & 0    \\
			-1/2 & 1    & -1   & 1/2  \\
			1/2  & -1   & 1    & -1/2 \\
			0    & 1/2  & -1/2 & 0
		\end{bmatrix}
	\end{equation*}
\end{example}

\subsubsection{Boundary Faces}
For $s\in \{0, N+1 \}$ and $x_s$ a boundary face we now have a smaller local contribution matrix
since the contribution can only come from the one element to which $x_s$ belongs. Meaning we have
$\widehat{\textbf{B}}_{\text{cons}}^{(0)}, \widehat{\textbf{B}}_{\text{cons}}^{(N+1)} \in \mathbb{R}^{(r+1)\times (r+1)}$
with
\begin{align*}
	 & [\widehat{\textbf{B}}_{\text{cons}}^{(0)}]_{i,j} =
	-\frac{2 c(x_0^+)}{h_{0}} \widehat{\phi}_j^{\,\prime}(-1) \widehat{\phi}_i(-1) - \frac{2 c(x_0^+)}{h_{0}} \widehat{\phi}_i^{\,\prime} (-1) \widehat{\phi}_j(-1)         \\
	 & [\widehat{\textbf{B}}_{\text{cons}}^{(N+1)}]_{i,j} =
	\frac{2 c(x_{N+1}^-)}{h_{N+1}} \widehat{\phi}_j^{\,\prime}(1) \widehat{\phi}_i(1) + \frac{ 2 c(x_{N+1}^-)}{h_{N+1}} \widehat{\phi}_i^{\,\prime} (1) \widehat{\phi}_j(1) \\
\end{align*}

\begin{example}
	For $c\equiv 1$ with $\mathcal{P}^1$-elements $(r=1)$ we have
	\begin{equation*}
		\widehat{\textbf{B}}_{\text{cons}}^{(0)} = \frac{1}{h_0}
		\begin{bmatrix}
			2  & -1 \\
			-1 & 0
		\end{bmatrix}
		,\qquad
		\widehat{\textbf{B}}_{\text{cons}}^{(N+1)} = \frac{1}{h_{N+1}}
		\begin{bmatrix}
			0  & -1 \\
			-1 & 2
		\end{bmatrix}
	\end{equation*}
\end{example}

\subsection{Assembly of B penalty part}
Again we rewrite $\textbf{B}_{\text{penal}} = \sum_{s=0}^{N+1} \textbf{B}_{\text{penal}}^{(s)}$, where
\begin{equation*}
	[\textbf{B}_{\text{penal}}^{(s)}]_{T(n,i), T(m,j)} = { \texttt{a}_s} \jump{\Phi_j^m(x_s)} \jump{\Phi_i^n(x_s)}
\end{equation*}
We proceed analogously to \ref{subsec:assembly_cons}.
\subsubsection{Interior Faces}
Let $s\in \{1,\ldots,N\} $ and $x_s$ denote an interior face. As before we have that
the entries of $ \textbf{B}_{\text{penal}}^{(s)} $ can only be nonzero for an index tuple
$ (T(n,i), T(m,j)) $ if $ n,m \in \{s, s-1\} $, similar to \ref{subsec:assembly_cons} we find
that the assembly boils down to adding up local contributions represented in a local contribution
matrix

\begin{equation*}
	\widehat{\textbf{B}}_{\text{penal}}^{(s)} =
	\begin{bmatrix}
		\textbf{C}_{\text{penal}}^{(s-1,s-1)} & \textbf{C}_{\text{penal}}^{(s-1,s)} \\
		\textbf{C}_{\text{penal}}^{(s,s-1)}   & \textbf{C}_{\text{penal}}^{(s,s)}
	\end{bmatrix}
	\in \mathbb{R}^{2(r+1) \times 2(r+1)}
\end{equation*}

and using (\ref{eq:shape_fun_and_basis_fun_values_equality}), and the definition of the penalization
parameter (\ref{def:penalization_function}) we specifically find

\begin{align*}
	 & [\textbf{C}_{\text{penal}}^{(s-1,s-1)}]_{i,j} = \texttt{a}_s \widehat{\phi}_j(1) \widehat{\phi}_i(1) \\
	 & [\textbf{C}_{\text{penal}}^{(s-1,s)}]_{i,j} = -\texttt{a}_s \widehat{\phi}_j(-1) \widehat{\phi}_i(1) \\
	 & [\textbf{C}_{\text{penal}}^{(s,s)}]_{i,j} = \texttt{a}_s \widehat{\phi}_j(-1) \widehat{\phi}_i(-1)
\end{align*}

where again by symmetry of the penalty term we have
$ \textbf{C}_{\text{penal}}^{(s-1,s)} = (\textbf{C}_{\text{penal}}^{(s,s-1)})^T$
and $\texttt{a}_s$ only depends on the two adjacent elements $I_{s-1}, I_s$.

\begin{example}
	Consider $c\equiv 1$ for $\mathcal{P}^1$-elements $(r=1)$
	with an equidistant mesh with meshsize $h$ we have
	\begin{equation*}
		\widehat{\textbf{B}}_{\text{penal}}^{(s)} = \frac{\sigma}{h}
		\begin{bmatrix}
			0 & 0  & 0  & 0 \\
			0 & 1  & -1 & 0 \\
			0 & -1 & 1  & 0 \\
			0 & 0  & 0  & 0
		\end{bmatrix}
	\end{equation*}
\end{example}

\subsubsection{Boundary Faces}
For $s \in  \{0, N+1\} $ we again have only the respective boundary element contributing.
So the local contribution matrices $ \widehat{\textbf{B}}_{\text{penal}}^{(s)} \in \mathbb{R}^{(r+1) \times (r+1)} $ satisfy

\begin{align*}
	[\widehat{\textbf{B}}_{\text{penal}}^{(0)}]_{i,j} =
	\texttt{a}_0 \widehat{\phi}_j(-1) \widehat{\phi}_i(-1), \qquad
	[\widehat{\textbf{B}}_{\text{penal}}^{(N+1)}]_{i,j} =
	\texttt{a}_{N+1} \widehat{\phi}_j(1) \widehat{\phi}_i(1) \\
\end{align*}

\begin{example}
	For $c\equiv 1$ with $\mathcal{P}^1$-elements $(r=1)$ we have
	\begin{equation*}
		\widehat{\textbf{B}}_{\text{penal}}^{(0)} = \frac{\sigma}{h_0}
		\begin{bmatrix}
			1 & 0 \\
			0 & 0
		\end{bmatrix}
		,\qquad
		\widehat{\textbf{B}}_{\text{penal}}^{(N+1)} = \frac{\sigma}{h_{N+1}}
		\begin{bmatrix}
			0 & 0 \\
			0 & 1
		\end{bmatrix}
	\end{equation*}
\end{example}

\section{System Vector Assembly}
We divide assembling the vector \textbf{l} in (\ref{eq:fully_discrete_dg_system_elliptic}) into two parts.
\begin{equation*}
	\textbf{l} = \textbf{l}_{\text{load}} + \textbf{l}_{\text{bc}}
\end{equation*}
First we recall the assembly of the load vector $\textbf{l}_{\text{load}}$, i.e. the vector containing the contributions of the forcing term $f$
and secondly we will describe how to add the Dirichlet boundary condition contributions ($\textbf{l}_{\text{bc}}$).
\subsection{Load Vector}
The assembly of the load vector is completely analogous to the continuous finite element case.
Using the local support of $\Phi_i^n$ we can rewrite

\begin{equation*}
	\int_{\Omega} f \Phi_i^n \text{d}x = \sum_{s=0}^{N} \int_{I_s} f \Phi_i^n \text{d} x = \int_{I_n} f(x) \Phi_i^n(x) \text{d} x
	= \frac{h_n}{2} \int_{-1}^{1} f(F_n(\xi))\widehat{\phi}_i(\xi)\text{d}\xi
\end{equation*}

meaning as before we can assemble $\textbf{l}_{\text{load}} = \sum_{s=0}^{N} \textbf{l}_{\text{load}}^{(s)}$ where
\begin{equation*}
    [\textbf{l}_{\text{load}}^{(s)}]_{T(n,i)} = \int_{I_s} f \Phi_i^n \text{d} x = \delta_{n,s} \frac{h_n}{2} \int_{-1}^{1} f(F_n(\xi))\widehat{\phi}_i(\xi)\text{d}\xi
\end{equation*}
which can be characterized by the local contribution vector $\widehat{\textbf{l}}_{\text{load}}^{\,(s)} \in \mathbb{R}^{r+1}$ defined as

\begin{equation*}
	[\widehat{\textbf{l}}_{\text{load}}^{\,(s)}]_{i}
	= \frac{h_s}{2} \int_{-1}^{1} f(F_s(\xi))\widehat{\phi}_i(\xi)\text{d}\xi
\end{equation*}
In practice we approximate the integral using a higher order Gauss-Lobatto quadrature rule.

\begin{example}
	For $f$ piecewise constant (i.e. $f|_{I_s} \equiv f_s \in \mathbb{R} \quad \forall s=0,\ldots,N$) with $\mathcal{P}^1$-elements $(r=1)$ we have
	\begin{equation*}
		\normalfont \widehat{\textbf{l}}_{\text{load}}^{\,(s)} = \frac{f_s h_s}{2}
		\begin{bmatrix}
			1 \\
			1
		\end{bmatrix}
	\end{equation*}
\end{example}

\subsection{Dirichlet Boundary Condition Vector}
We have

\begin{equation*}
	[\textbf{l}_{\text{bc}}]_{T(n,i)} =
	-g_1c(x_{N+1}^-)\Phi_i^{n \, \prime}(x_{N+1}^-) + g_0c(x_0^+)\Phi_i^{n \, \prime}(x_0^+)
	+ \texttt{a}_{N+1}g_1\Phi_i^n(x_{N+1}^-) + \texttt{a}_0 g_0\Phi_i^n(x_{0}^+)
\end{equation*}

where the entries can clearly only be non-zero at indices corresponding to
boundary elements.
We characterize the assembly using the local contribution vectors
$\widehat{\textbf{l}}_{\text{bc}}^{\,(N+1)}, \widehat{\textbf{l}}_{\text{bc}}^{\,(0)} \in \mathbb{R}^{r+1}$
where

\begin{equation*}
	[\widehat{\textbf{l}}_{\text{bc}}^{\,(0)}]_{i} = \frac{g_0}{h_0} c(x_{0}^-) \widehat{\phi}_i(-1) + \texttt{a}_{0}g_0\widehat{\phi}_i(-1), \qquad
	[\widehat{\textbf{l}}_{\text{bc}}^{\,(N+1)}]_{i} = -\frac{g_1}{h_N} c(x_{N+1}^-) \widehat{\phi}_i(1) + \texttt{a}_{N+1}g_1\widehat{\phi}_i(1)
\end{equation*}


\section{Mass Matrix Assembly}
Although the mass matrix has not yet appeared in the discrete formulation as presented in (\ref{eq:fully_discrete_dg_system_elliptic}) due to the absence
of a mass term in the elliptic pde as presented in (\ref{eq:elliptic_pde}), we will require it soon and hence quickly review it's assembly here.
\\ 
The mass matrix is defined as 
\begin{equation*}
	[\mathbf{M}]_{T(n,i), T(m,j)} = (\Phi_j^m, \Phi_i^n)_{L^2(\Omega)} = \sum_{s = 0}^{N} \int_{I_s} \Phi_j^m \Phi_i^n \text{ d}x.
\end{equation*}
So we can rewrite $\mathbf{M} = \sum_{s=0}^N \mathbf{M}^{(s)}$, where 
\begin{equation*}
	[\mathbf{M}^{(s)}]_{T(n,i), T(m,j)} = \int_{I_s}  \Phi_j^m \Phi_i^n \text{ d}x.
\end{equation*}  
Similarly to before the entries of $\mathbf{M}^{(s)}$ are only non-zero if $m = n = s$. In that case we pull the integral back to the reference element aby substitution
and find 
\begin{equation*}
	\int_{I_s}  \Phi_j^m \Phi_i^n \text{ d}x = \frac{h_s}{2} \int_{-1}^{1} \widehat{\phi}_j \widehat{\phi}_i \text{ d}x.
\end{equation*}
The assembly of $\mathbf{M}$ corresponds therefore to summing up local contribution matrices $ {\widehat{\mathbf{M}}}^{(s)} \in \R ^{(r+1) \times (r+1)}$.
In fact since the elements are decoupled due to the discontinuity of the basis functions, the resulting matrix is a block-diagonal matrix where each block corresponds to the local
contribution matrix $ \widehat{\mathbf{M}}^{(s)}$

\begin{example}
	For $\mathcal{P}^1$-elements $(r=1)$ we have
	\begin{equation*}
		\widehat{\textbf{M}}^{(s)} = \frac{h_s}{6}
		\begin{bmatrix}
			2  & 1 \\
			1 & 2
		\end{bmatrix}.
	\end{equation*}
\end{example}